{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data Preprocess steps and Modeling with XGboost\n",
    "\n",
    "To build a data pipeline flexible enough to accormodate future data changes \n",
    "1. Build modules for each data prepartion steps, easier to scale \n",
    "2. Handling special dirty data (such as negative values, and exterme outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key decisions from data understanding**\n",
    "1. remove \"last data vet visit\" and index column from model building\n",
    "2. Handle exterme outliers and negative values,the one beyond quantile 99.9%, such as the one with hair length, but keep the others \n",
    "3. Keep the dirty data, such as \"Die of age\" 0, the future data might have the same kind of data, so the model training need to take that into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed cloudpickle-1.6.0 future-0.18.2 hyperopt-0.2.5 networkx-2.5\n"
     ]
    }
   ],
   "source": [
    "! pip install hyperopt | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv file\n",
    "def load_data(filepath):\n",
    "    \n",
    "    columns = ['index','age','breed','last_vet_visit','hair_length','height','num_vet_visit','weight']\n",
    "    \n",
    "    return pd.read_csv(filepath,names=columns,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all negative values from data\n",
    "def remove_negative_value(df, columns):\n",
    "    # df: dataframe of the data \n",
    "    # columns: list of columns need to remove negative value from \n",
    "    \n",
    "    if len(columns) == 0 and df.empty:\n",
    "        return df\n",
    "    \n",
    "    for col in columns:\n",
    "        df = df[df[col]>=0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extreme_outliers(df,columns):\n",
    "    #for values great then quantile 99.9% then it would be treated as exterm outlier \n",
    "    # df: dataframe of the data \n",
    "    # columns: list of columns need to remove negative value from \n",
    "    \n",
    "    if len(columns) == 0 and df.empty:\n",
    "        return df\n",
    "    \n",
    "    for col in columns:\n",
    "        limit = df[col].quantile(0.999)\n",
    "        df = df[df[col]<limit] \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Data preprocess and split testing and training set\n",
    "\n",
    "def preprocess():\n",
    "    \n",
    "    filepath='data.csv'\n",
    "\n",
    "    df = load_data(filepath)\n",
    "\n",
    "    #remove  column \n",
    "    df = df.drop(columns=['index','last_vet_visit'])\n",
    "\n",
    "    #remove any negative value\n",
    "    df = remove_negative_value(df,['age','hair_length','height','num_vet_visit','weight'])\n",
    "\n",
    "    #remove any exterme outliers\n",
    "    df = remove_extreme_outliers(df,['age','hair_length','height','num_vet_visit','weight'])\n",
    "    \n",
    "    #split training, test and validation set\n",
    " \n",
    "    test_size = 0.2\n",
    "    \n",
    "    df_train,df_test = train_test_split(df, test_size=test_size)\n",
    "\n",
    "    \n",
    "    return df_train,df_test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (232, 6), test size (58, 6)\n"
     ]
    }
   ],
   "source": [
    "train,test=preprocess()\n",
    "print('train size {}, test size {}'.format(train.shape,test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-41cf55a16b05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "# further data processing, standardise the numerical columns, and perform one hot encoding for category values \n",
    "# On the category values, for those category with exterme less examples, such as 'Donald'\n",
    "# consider rename all those categories into a common name, such as \"Other\"\n",
    "    \n",
    "category_cols = ['breed']\n",
    "numerical_cols = ['hair_length','height','num_vet_visit','weight']\n",
    "\n",
    "transformer = make_pipeline(\n",
    "    ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), category_cols),    \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data process\n",
    "\n",
    "df_train,df_test = preprocess()\n",
    "\n",
    "age_train = df_train.age.copy().to_numpy()\n",
    "age_test = df_test.age.copy().to_numpy()\n",
    "\n",
    "\n",
    "x_train = transformer.fit_transform(df_train.drop(['age'],axis=1))\n",
    "x_test = transformer.transform(df_test.drop(['age'],axis=1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter turning\n",
    "\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "def objective(space):\n",
    "    reg = XGBRegressor(n_jobs=-1, \n",
    "                           eval_metric=\"rmse\", \n",
    "                           eta=space[\"eta\"], \n",
    "                           max_depth=space[\"max_depth\"], \n",
    "                           n_estimators=space[\"n_estimators\"], \n",
    "                           min_child_weight=space[\"min_child_weight\"], \n",
    "                           colsample_bytree=space[\"colsample_bytree\"],\n",
    "                           subsample=space[\"subsample\"], \n",
    "                           seed=1,\n",
    "                          silent=True)\n",
    "    reg.fit(x_train, age_train, eval_set=[(x_train, age_train), (x_test, age_test)], early_stopping_rounds=200, verbose=False) \n",
    "    age_pred = reg.predict(x_test)\n",
    "    mse = mean_squared_error(age_test, age_pred)\n",
    "    return {\"loss\": mse, \"status\": STATUS_OK }\n",
    "\n",
    "space = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 3, 8, q=1)),\n",
    "    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 150, 450, q=50)),\n",
    "    \"eta\": hp.quniform(\"eta\", 0.05, 0.2, 0.05),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 0.5, 1.8, 0.1),\n",
    "    \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.1),\n",
    "    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.5, 1, 0.1)\n",
    "}\n",
    "\n",
    "best = fmin(\n",
    "    fn = objective,\n",
    "    space = space,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 1000,\n",
    "    rstate = np.random.RandomState(12345)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model withe identified hyperparameter\n",
    "model = XGBRegressor(n_jobs=-1, \n",
    "                   eval_metric=\"rmse\",\n",
    "                   subsample=0.8,\n",
    "                   colsample_bytree=0.7,\n",
    "                   eta=0.2,\n",
    "                   max_depth=4,\n",
    "                   min_child_weight=1.6,\n",
    "                   n_estimators=400,\n",
    "                    seed=1)\n",
    "\n",
    "model.fit(x_train,age_train,eval_set=[(x_train, age_train), (x_test, age_test)], early_stopping_rounds=200, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = model.predict(x_test)\n",
    "mean_squared_error(predict_test,age_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the pipeline and model\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "dump(model, 'model.joblib')\n",
    "dump(transformer,'transformer.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
